# 딥러닝 1주차 {.unnumbered}

머신러닝 vs. 딥러닝?

암튼 머신러닝이란건 기계가 스스로 학습해서 문제를 해결한다는 의미.

우리가 컴퓨터에게 적정한 모델을 입력하면, 해당 모델을 가지고 주어진 데이터를 반복계산하여 적절한 결과값을 도출.

딥러닝은 머신러닝의 한 분류로, 적정한 모델을 인공신경망 기반의 모델을 사용한 것을 의미한다고 보면 됨.

인공신경망이 뭔지는 아직 모름. 매우 복잡하고, 적절한 결과값 도출 과정을 해석하기 어렵고, 예측력은 매우 높음.

학습데이터가 매우 많이 필요하고, 많은 계산시간, 모델 탐색시간, 하이퍼파라미터 조율시간 등이 필요함.

오류에 대한 디버깅이나 해석도 어렵다고 함.

## 머신러닝 알고리즘의 구분

### 지도학습

입력값에 대한 결과값이 주어진 경우, 

모델이 입력값과 결과값을 모두 알고 학습하여 새로운 입력값에 대한 적정 결과값 추정치를 제공하게 됨.

결과값(label)이 숫자형이면 회귀(regression) 알고리즘, 범주형이면 분류(classification) 알고리즘으로 구분.

### 비지도학습

결과값이 없고, 주어진 입력값만으로 학습함.

의미있는 패턴을 추출하는 것이 목적.

군집화(행을 묶음) 및 차원축소(열을 묶어 열의 갯수를 감소시킴)에 주로 활용

### 강화학습

모델 자체가 어떠한 변화를 주도하는데,

해당 변화에 따른 보상/패널티를 주는 환경을 구성함.

모델은 이러한 변화에 따른 누적보상이 최대가 되도록하는 변화패턴을 학습함

## 지도학습 알고리즘의 절차

1. 전처리 및 탐색

2. 적절한 모델 선택

3. 주어진 데이터로 모델 훈련

4. 새로운 데이터를 통해 결과값을 예측하여 모델의 성능을 평가

### 경사하강법 (Gradient Descent Method)

다차원 선형회귀 모형에서 모델결과값과 실제결과값의 차이(MSE; Mean Square Error)를 최소화하는 방식

MSE의 미분계수(Gradient)의 일정수준(학습률)만큼 선형회귀모형의 각 파라미터가 모두 감소하도록 지속 업데이트하면서,

결과적으로 MSE의 Gradient가 0이 되면 학습이 종료되는 방식.

MSE의 미분계수가 0이면 최솟값이고, 모델결과값의 오차가 최소가 되므로 가장 적정한 파라미터를 추론한 것으로 판단.

#### 경사하강법 종류

한번의 회귀계수 업데이트에 모든 훈련데이터를 사용하면 배치 경사하강법

임의추출로 하나의 훈련데이터만 사용하면 확률 경사하강법(SGD)

일부만 사용하면 미니 경사하강법.

많이 사용할수록 규칙적으로 MSE가 감소하고 일관적으로 움직이나, 산출시간이 오래걸리고, 지역최소값에 갇힐 가능성이 높아짐.

## 모델 평가 및 검증

low bias - high vol

high bias - low vol

제대로 못들어서 정리 필요

### 자료의 구분

일반적으로 전체 데이터를 임의로 3개로 나눔.

훈련 데이터 / 검증 데이터 / 평가 데이터

e.g. 훈련데이터로 여러 h-para에 대해 모델 돌림

검증데이터를 이용해 각 h-para별 성능 평가

제일 좋은 h-para로 모델을 구성하여 훈련+검증데이터로 다시 훈련

최종 모델을 평가데이터를 이용하여 평가

----

딥러닝에서는 검증데이터가 다른 의미로도 활용됨.

경사하강법 같은걸 복잡한 모델에 사용할 때, 파라미터 튜닝 과정에서 손실함수가 더이상 감소하지 않는다면?

불필요한 훈련이 될 수 있어 파라미터 자체가 학습이 잘 되고 있는지 모니터링을 해야 함.

이러한 모니터링에 검증 데이터가 활용됨.

### 모델의 평가를 위한 지표

#### 회귀모형, Regression model

**RMSE** : $\sqrt{MSE}$

**MAE**(mean absolute error) : $\frac{1}{n}\sum_{i=1}^n|y_i-\hat{y}_i|$

**R square**(결정계수) : $1-\frac{\sum_{i=1}^n(y_i-\hat{y}_i)^2}{\sum_{i=1}^n(y_i-\bar{y})^2}=1-\frac{SSE}{SST}(=\frac{SSR}{SST})$

#### 분류모형

**정오분류표** : TN(true negative), TP(true positive), FN, FP

**정확도, 정분류율** (Accuracy) : $\frac{TN+TP}{TN+TP+FN+FP}$

**오분류율** : 1 - 정분류율

**교차엔트로피 오차**(Cross Entropy Error), Multiclass classification에 많이 씀

$$-\frac{1}{n}\sum_{i=1}^n\sum_{k=1}^Ky_k^{(i)}log(\hat{p}_k^(i))$$

y는 타깃확률, p는 예측확률, K는 범주의 개수

K=2인 경우, 위의 교차엔트로피 손실함수를 로그손실함수라고 부르며, 많이 활용함.

::: {.callout}
삼중분류문제의 경우,

모델은 훈련자료를 기반으로 훈련 후 0,1,2에 각각 속할 확률을 계산하여 반환함.($p_0+p_1+p_2=1$)

최종적으로 각 확률중 가장 큰 값을 $\hat{y}$로 산출하게 됨.

교차 엔트로피의 타깃 확률이란 1을 의미하며, 실제 y가 특정 범주 k에 속할 때 1이며, 아닐 때 0임.

따라서, 교차엔트로피의 수식을 볼 때 타깃확률이 1이고 예측확률이 1에 가까울수록 오차는 0에 수렴하며

예측확률이 1보다 작을수록 오차는 커지게 됨
:::