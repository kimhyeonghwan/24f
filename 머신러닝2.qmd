# 머신러닝 Ch2-3 {.unnumbered}

선형회귀모형

비용함수 : $J(\theta)=\frac{1}{2m}\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2$

$\theta^*=\theta-\alpha\,\triangledown_\theta J(\theta)$

$\triangledown_\theta J(\theta)=\frac{1}{m}X^T(X\theta-y)$

배치 / 확률적 / 미니배치

정규방정식에서 $\hat{\theta}=(X^TX)^{-1}X^Ty$의 closed form 존재.

역행렬 계산에 많은 시간이 소요되는 경우에는 사용이 곤란

## 규제가 있는 선형회귀

### 릿지회귀모형

L2 규제를 사용하여 파라미터의 범위를 제약

비용함수 : $J(\theta)=\frac{1}{2m}(\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2)$

파라미터 추정치는, $\hat\theta^R=arg\min_\theta J(\theta)$

행렬식 : $\hat\theta^R=arg\min_\theta\{\frac{1}{2m}(X\theta-y)^T(X\theta-y)+\lambda\theta_1^T\theta_1\}$

Alter : $\hat\theta^R=arg\min_\theta\{\frac{1}{2m}(X\theta-y)^T(X\theta-y)\}\;subject\;to\;\sum_{j=1}^n\theta_j^2\leq t$

즉, 파라미터 제곱합에 일정 상한을 정해놓는 방식으로 보면 됨.

람다가 커지면? 파라미터 제약이 커지면서 편향이 증가함

#### 파라미터 추정치 계산

정규방정식 : $\hat\theta^R=(X^TX+\lambda I)^{-1}X^Ty$

일반 회귀모형에서는 $X^TX$가 singular이면 해가 없었으나, 릿지제약 하에서는 이를 해결할 수 있음.

또한, 다중공선성(multicolinearity)도 해결할 수 있다는 것이 알려져 있음.

경사하강법 : $\theta_{new}=\theta_{old}-\alpha\,\triangledown J(\theta)=(1-\alpha\frac{\lambda}{m})\theta_{old}-\alpha\,\triangledown J^U(\theta)$

마지막 식은, 일반선형회귀의 그레디언트를 이용해서 표현한 것이며, 직전 $\theta_{old}$의 영향을 상수배로 줄여주는 것으로 해석할 수 있음.

### 라쏘회귀모형

L1 규제를 이용. Least Absolute Shrinkage & Selection Operator

모델로 널리 이용되지는 않지면, feature가 너무 많은 경우 적절한 변수만 선택할 때 많이 이용됨.

비용함수 : $J(\theta)=\frac{1}{2m}(\sum_{i=1}^m(\theta^Tx^{(i)}-y^{(i)})^2+\lambda\sum_{j=1}^n|\theta_j|)$

파라미터 추정치는, $\hat\theta^L=arg\min_\theta J(\theta)$

행렬식 : $\hat\theta^L=arg\min_\theta\{\frac{1}{2m}(X\theta-y)^T(X\theta-y)+\lambda 1^T|\theta_1|\}$

Alter : $\hat\theta^L=arg\min_\theta\{\frac{1}{2m}(X\theta-y)^T(X\theta-y)\}\;subject\;to\;\sum_{j=1}^n|\theta_j|\leq t$

함수꼴에서 보듯, 미분불가능하므로 closed form solution이 존재하지 않음.

다만, $\theta$의 부호에 따라 절대값의 미분값은 -1 또는 1이므로 첨값의 미분값이 0이라고 가정하면 다음과 같이 표현가능함.

$\theta_{new}\leftarrow \theta_{old}-\alpha\,\triangledown J(\theta_{old})=\theta_{old}-\alpha\bigl(\triangledown J^U(\theta_{old})-\lambda_{(=\frac{\lambda}{2m})}\;sign(\theta_{old})\bigr)$

$for\;sign(\theta)=\frac{|\theta|}{\theta}\;or\;0\;where\;\theta=0$

즉, 원점을 향해 일정부분(\theta) 계속 보정이 들어가는 형태.

### 비교

릿지회귀모형 vs. 라쏘회귀모형

    해석력 : 라쏘
    예측력 : 기본적으로, 규제가 생기면 예측력이 증가하긴 함. 어떤 모형이 예측력이 뛰어난지는 데이터에 따라 달려있음

만약 feature의 수가 샘플의 수보다 작으면, 라쏘를 쓰면 안됌