# 딥러닝 Ch2 {.unnumbered}

인공 신경망 모형 (Neural Network)

## 퍼셉트론 (Perceptron)

하나의 인공뉴런으로 구성된 신경망. 가장 기본적인 구조.

주어진 데이터로 분류면을 찾는 것이 목표

### 로젠블랜의 퍼셉트론 (Simple perceptron)

활성화함수 $f(.)$를 1 또는 -1를 가지는 임계함수로 구성.

y = 1인데 f=-1인 경우, $w_i^*=w_i+x_i$

y = -1인데 f=1인 경우, $w_i^*=w_i-x_i$

즉, 실제는 1인데 $s=\sum w_ix_i<0$이 되어 임계가 -1로 출력되는 경우,

$\sum (w_i^*x_i+b^*_{(w^*_0x_0)})=\sum (w_ix_i+x_i^2+b)$이 되어 s가 증가하는 방향으로 움직이게 됨.

### 선형 퍼셉트론 (Linear perceptron)

활성화함수로 선형함수를 사용함 $f(s)=s$

파라미터 학습시 델타규칙(delta rule)을 사용하는데, 경사하강법(GDM)으로 보면 됨

N개의 훈련자료 $D_N=\{(x^{(d),y^{(d)}})\}_{d=1}^N$에 대해 $f^{(d)}=\sum_i w_ix_i^{(d)}$가 되며,

학습오차 $E_d=\frac{1}{2}(y^{(d)}-f(s)^{(d)})^2$ 및 $E_N=\sum E_d$

(N으로 나누던 안나누던 파라미터 업데이트 결정에는 영향 없음)

여기에 SGD(Stochastic Gredient Descent)를 적용하여 파라미터 업데이트

$w_i\leftarrow w_i+\Delta w_i=w_i-\eta \frac{\partial E_d}{\partial w_i}=w_i+\eta(y^{(d)}-f^{(d)})x_i$

::: {.callout}
초기 델타규칙의 파라미터 업데이트는 경사하강법의 표현법이 아니였음.

$w_i\leftarrow w_i+\eta ex_i\;,\;e=y-x_i$ 방식으로 업데이트.

결국 목적함수를 미분하면 해당 꼴이 나타나므로 경사하강법과 동일한 논리임.
:::

### 시그모이드 퍼셉트론 (Sigmoid perceptron)

머신러닝의 logistic regression과 거의 유사함. (비용함수만 조금 다름)

활성함수를 시그모이드 함수 $f=\frac{1}{1+e^{-s}}\in (0,1)$로 사용함.

즉, y가 이진분류 문제일 때 y=1일 확률값을 예측해주는 모델임.

어차피 퍼셉트론에서는 확률=0.5를 기준으로 분류하므로 선형분류면을 제공하지만,

네트워크를 구성하면 비선형 경계면을 구성할 수도 있음.

$f^{(d)}=\sigma(s^{(d)})=\frac{1}{1+e^{-s^{(d)}}}=\frac{1}{1+e^{-\sum w_ix^{(d)}_i}}$

$E_d=\frac{1}{2}(y^{(d)}-\sigma(s^{(d)}))^2$

$\frac{\partial E_d}{\partial w_i}=(y^{(d)}-\sigma^{(d)})\sigma^{(s)}(1-\sigma^{(s)})(-x_i^{(d)})$

$\therefore w_i^*\;\leftarrow\;w_i+\eta (y^{(d)}-f^{(d)})\,f^{(s)}\,(1-f^{(s)})x_i^{(d)}$

$w_i$의 업데이트는 동시에 이루어져야함에 유의

::: {.callout}
시그모이드 함수 미분

$\frac{d\sigma(s)}{ds}=\frac{+e^{-s}}{(1+e^{-s})^2}=\sigma(s)(1-\sigma(s))$
:::

## 다층 퍼셉트론 (Multi-layer perceptron)

XOR문제 : $(x_1,x_2,y)=(1,0,1),\;(0,1,1),\;(0,0,0),\;(1,1,0)$

기존 퍼셉트론은 이 간단한 XOR문제도 해결할 수 없음.

여러개의 퍼셉트론을 여러 층으로 쌓는다면 이를 해결 가능함.

선을 두개를 그어서 $z_1,z_2$를 구성하고, $f=z_1\times z_2$를 최종 모델로 결정하면 문제 해결 가능.