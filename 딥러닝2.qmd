# 딥러닝 Ch2 {.unnumbered}

인공 신경망 모형 (Neural Network)

## 퍼셉트론 (Perceptron)

하나의 인공뉴런으로 구성된 신경망. 가장 기본적인 구조.

주어진 데이터로 분류면을 찾는 것이 목표

### 로젠블랜의 퍼셉트론 (Simple perceptron)

활성화함수 $f(.)$를 1 또는 -1를 가지는 임계함수로 구성.

y = 1인데 f=-1인 경우, $w_i^*=w_i+x_i$

y = -1인데 f=1인 경우, $w_i^*=w_i-x_i$

즉, 실제는 1인데 $s=\sum w_ix_i<0$이 되어 임계가 -1로 출력되는 경우,

$\sum (w_i^*x_i+b^*_{(w^*_0x_0)})=\sum (w_ix_i+x_i^2+b)$이 되어 s가 증가하는 방향으로 움직이게 됨.

### 선형 퍼셉트론 (Linear perceptron)

활성화함수로 선형함수를 사용함 $f(s)=s$

파라미터 학습시 델타규칙(delta rule)을 사용하는데, 경사하강법(GDM)으로 보면 됨

N개의 훈련자료 $D_N=\{(x^{(d),y^{(d)}})\}_{d=1}^N$에 대해 $f^{(d)}=\sum_i w_ix_i^{(d)}$가 되며,

학습오차 $E_d=\frac{1}{2}(y^{(d)}-f(s)^{(d)})^2$ 및 $E_N=\sum E_d$

(N으로 나누던 안나누던 파라미터 업데이트 결정에는 영향 없음)

여기에 SGD(Stochastic Gredient Descent)를 적용하여 파라미터 업데이트

$w_i\leftarrow w_i+\Delta w_i=w_i-\eta \frac{\partial E_d}{\partial w_i}=w_i+\eta(y^{(d)}-f^{(d)})x_i$

::: {.callout}
초기 델타규칙의 파라미터 업데이트는 경사하강법의 표현법이 아니였음.

$w_i\leftarrow w_i+\eta ex_i\;,\;e=y-x_i$ 방식으로 업데이트.

결국 목적함수를 미분하면 해당 꼴이 나타나므로 경사하강법과 동일한 논리임.
:::

### 시그모이드 퍼셉트론 (Sigmoid perceptron)

머신러닝의 logistic regression과 거의 유사함. (비용함수만 조금 다름)

활성함수를 시그모이드 함수 $f=\frac{1}{1+e^{-s}}\in (0,1)$로 사용함.

즉, y가 이진분류 문제일 때 y=1일 확률값을 예측해주는 모델임.

어차피 퍼셉트론에서는 확률=0.5를 기준으로 분류하므로 선형분류면을 제공하지만,

네트워크를 구성하면 비선형 경계면을 구성할 수도 있음.

$f^{(d)}=\sigma(s^{(d)})=\frac{1}{1+e^{-s^{(d)}}}=\frac{1}{1+e^{-\sum w_ix^{(d)}_i}}$

$E_d=\frac{1}{2}(y^{(d)}-\sigma(s^{(d)}))^2$

$\frac{\partial E_d}{\partial w_i}=(y^{(d)}-\sigma^{(d)})\sigma^{(s)}(1-\sigma^{(s)})(-x_i^{(d)})$

$\therefore w_i^*\;\leftarrow\;w_i+\eta (y^{(d)}-f^{(d)})\,f^{(s)}\,(1-f^{(s)})x_i^{(d)}$

$w_i$의 업데이트는 동시에 이루어져야함에 유의

::: {.callout}
시그모이드 함수 미분

$\frac{d\sigma(s)}{ds}=\frac{+e^{-s}}{(1+e^{-s})^2}=\sigma(s)(1-\sigma(s))$
:::

## 다층 퍼셉트론 (Multi-layer perceptron)

XOR문제 : $(x_1,x_2,y)=(1,0,1),\;(0,1,1),\;(0,0,0),\;(1,1,0)$

기존 퍼셉트론은 이 간단한 XOR문제도 해결할 수 없음.

여러개의 퍼셉트론을 여러 층으로 쌓는다면 이를 해결 가능함.

선을 두개를 그어서 $z_1,z_2$를 구성하고, $f=z_1\times z_2$를 최종 모델로 결정하면 문제 해결 가능.

### 다층 퍼셉트론의 구조

여러 개의 퍼셉트론 뉴런을 여러 층으로 쌓은 구조이며, 층 내에서 뉴런간 교류는 없음.

입력 - 은닉 - 출력 다층퍼셉트론이 대표적인 구조. (I-H-K 다층퍼셉트론)

입력층, 은닉층을 거쳐 출력층의 연산을 통해 최종적으로 값을 출력

파라미터 학습은 각 층마다 가중치로 존재하므로, 실질적으로 층이 2개인 MLP라고 하기도 함.

::: {.callout}
e.g. 10-3-1 MLP는?

10개의 입력층(+1 bias항), 3개의 은닉층(+1 bias항), 1개의 출력층

총 11개의 입력층과 3개의 은닉층이 모두 연결되어 총 33개의 연결이 생기고,

4개의 은닉층과 1개의 출력층이 연결되어 4개의 연결이 생김. 전체 37개의 연결.
:::

층의 개수나 뉴런의 개수를 모두 돌려봐서 최적의 모델을 찾으면 좋지 않나?

그러나, 딥러닝 모델은 하나의 학습에 많은 시간이 걸려서 그런식으로 하지 않고, 유사 사례를 참고하는 등 적정 모델을 초기부터 잘 설정해야할 필요가 있음.

### 다층 퍼셉트론의 학습

#### 전향계산 Forward Calculation

각 층간에는 퍼셉트론과 동일한 연산이 적용되며, 따라서 층이 많아질수록 계산이 기하급수적으로 증가.

![](image/deeplearning_MLP.png)

I-H-K MLP에서, I개의 입력층과 H개의 은닉층 사이에 $H\times (I+1)$개의 가중치가 필요하며,

은닉층과 출력층 사이에 $K\times (H+1)$개의 가중치가 필요.

이러한 가중치를 학습할 때 어떻게하는지? 오류역전파 알고리즘 사용

#### 오류역전파 알고리즘 Error back-propagation

