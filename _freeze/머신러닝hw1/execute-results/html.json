{
  "hash": "d4ccd4eac4f42eff7b135a4bff00f527",
  "result": {
    "engine": "jupyter",
    "markdown": "# 인공지능 및 기계학습 과제1 {.unnumbered}\n\n20249132 김형환\n\n## Question 1\n\n![](image/machine_hw1_1.png)\n\n### Answer\n\n::: {#74613938 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\n\nnp.random.seed(123)\nxtrain = 2 * np.random.rand(100, 3)\nytrain = 6 + xtrain @ np.array([[3],[2],[5]]) + np.random.randn(100, 1)\nxtest = 2 * np.random.rand(20, 3)\nytest = 6 + xtest @ np.array([[3],[2],[5]]) + np.random.randn(20,1)\n\n# (1)\ndef gradient_descent_steps(xtrain, ytrain, iters):\n    # m = 훈련데이터의 수, n = 변수의 개수\n    m, n = xtrain.shape\n    # 절편항 추가를 위해 x0=1을 각 훈련데이터에 추가\n    X = np.insert(xtrain, 0, 1, axis = 1)\n    Y = ytrain\n    # 파라미터 theta의 초기값은 1, 절편항까지 n+1개 \n    theta = np.ones(n+1).reshape(n+1,1)\n    for i in range(iters):\n        # 비용함수 = (오차의 제곱합) / 2m 형태로 가정 \n        gradient = X.T @ (X @ theta - Y) / m\n        theta -=0.01 * gradient\n    return theta\n\nw_pred = gradient_descent_steps(xtrain, ytrain, iters=5000)\nprint(\"Best parameters are :\")\nprint( w_pred )\n\n# (2)\nypred = np.insert(xtest,0,1,axis = 1) @ w_pred\nmse = mean_squared_error(ytest, ypred)\nprint(\"Mean Squared Error is :\")\nprint(mse)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest parameters are :\n[[5.72120369]\n [3.00637126]\n [2.29860859]\n [4.9014743 ]]\nMean Squared Error is :\n0.8604989666540144\n```\n:::\n:::\n\n\n## Question 2\n\n![](image/machine_hw1_2.png)\n\n![](image/machine_hw1_3.png)\n\n::: {#b823e848 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\n\nhousing = fetch_california_housing()\n\n# (1)\ny = housing['target']\nx = pd.DataFrame( housing['data'], columns=housing['feature_names'] )\n\n# train_test_split 모듈 이용\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.4, random_state=123)\nprint(\"Split data to training 0.6 and test 0.4 ratio\")\nprint(\"Data size of X train, test : \",x_train.shape[0], x_test.shape[0])\nprint(\"Data size of y train, test : \",y_train.shape[0], y_test.shape[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSplit data to training 0.6 and test 0.4 ratio\nData size of X train, test :  12384 8256\nData size of y train, test :  12384 8256\n```\n:::\n:::\n\n\n::: {#746037e0 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\n\n# (2) 표준화 후 GridSearchCV 모듈 활용. 하이퍼파라미터 튜닝 및 재훈련.\nscaler = StandardScaler()\nscaler.fit( x_train )\nx_train = scaler.transform( x_train )\nx_test = scaler.transform( x_test )\n\nparams = {'alpha': [0, 1, 10, 30, 50, 100]}\nhousing_grid_ridge = GridSearchCV ( Ridge(),\n                                    param_grid=params,\n                                    cv=5, \n                                    scoring='neg_mean_squared_error', \n                                    refit=True)\nhousing_grid_ridge.fit( x_train, y_train )\nprint(\"The best lambda(alpha) is :\",housing_grid_ridge.best_params_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe best lambda(alpha) is : {'alpha': 1}\n```\n:::\n:::\n\n\n::: {#d58ce79c .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.metrics import r2_score\n\n# (3) GridSearchCV 결과값 및 r2_score 함수 활용\nhousing_ridge = housing_grid_ridge.best_estimator_\ny_pred = housing_ridge.predict( x_test )\nR2 = r2_score( y_test, y_pred )\n\nprint(\"The coefficients are : \",housing_ridge.coef_,\"\\n\")\nprint(\"R-square is :\", R2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe coefficients are :  [ 0.83361052  0.11707262 -0.28230537  0.33026218  0.00106348 -0.04327138\n -0.89953142 -0.87471626] \n\nR-square is : 0.6090749286505223\n```\n:::\n:::\n\n\n::: {.callout}\nGridSearchCV 대신 cross_val_score를 이용해도 동일한 결과를 얻을 수 있습니다.\n\n::: {#4fb983ab .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\nparams = [0, 1, 10, 30, 50, 100]\nmse = np.ones(6)\n\nfor al in range(6):\n    ridge = Ridge(alpha=params[al])\n    neg_mse_scores = cross_val_score(ridge, x_train, y_train,\n    scoring='neg_mean_squared_error', cv=5)\n    avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores))\n    mse[al] = avg_rmse\n\nalpha_star = params[mse.argmin()]\nridge = Ridge(alpha=alpha_star)\nridge.fit( x_train, y_train )\nprint(\"alpha is : \", alpha_star)\nprint(\"coef. is : \", ridge.coef_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nalpha is :  1\ncoef. is :  [ 0.83361052  0.11707262 -0.28230537  0.33026218  0.00106348 -0.04327138\n -0.89953142 -0.87471626]\n```\n:::\n:::\n\n\n:::\n\n## Question 3\n\n![](image/machine_hw1_4.png)\n\n### Answer : 3번\n\n**기본적으로 규제가 있는 회귀모형은 일반적인 회귀모형에 비하여 유연성이 떨어집니다.**\n\n비용함수를 최소화시켜나가는 과정에서 파라미터가 만족해야할 추가적인 조건이 붙기 때문입니다.\n\n이를 알기 쉽게 경사하강법을 예시로 들어보면, 매 파라미터를 업데이트해나갈 때,\n\n**규제가 있는 회귀모형에서는 규제로 인해 업데이트를 원하는 만큼 실시할 수 없게** 됩니다.\n\n따라서 규제가 조금이라도 존재한다면, 일반적인 회귀모형에 비해서 유연성이 떨어집니다.\n\n**유연성은 떨어지는 대신 장점이 있는데, 모델의 overfitting 문제를 잘 해결**한다는 것 입니다.\n\n모델이 train data로 훈련할 때 파라미터에 제약을 가하기 때문에, 모델이 과적합되지 않도록 적정히 조절하게 됩니다.\n\n**이를 종합해보면, 규제가 있는 회귀모형은 유연성이 떨어져 다소 편향이 발생하게 되지만,**\n\n**과적합을 방지함으로써 분산이 감소되는 효과 있다는 의미**가 됩니다.\n\n따라서 규제의 여부와 규제의 정도를 결정할 때, 이러한 trade-off를 잘 이해하여야 합니다.\n\n**분산 감소효과가 편향 증가효과보다 클 때, 규제를 가하거나 규제의 정도를 강화하는 것이 적합**할 것 입니다. ***(3번)***\n\n",
    "supporting": [
      "머신러닝hw1_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}