{
  "hash": "7b358ca8922bd394711021966a14aba7",
  "result": {
    "engine": "jupyter",
    "markdown": "# 시뮬레이션방법론 Ch1 {.unnumbered}\n\n몬테카를로 시뮬레이션 기초\n\n## 블랙숄즈공식 예시\n\n$1.\\;f_t+\\frac{1}{2}\\sigma^2S^2f_{ss}+rSf_s-rf=0$\n\n-\\> 수치해석적인 방법으로 풀게 됨, FDM(Finite Difference Method)\n\n$2.\\;P(0)=e^{-rT}E^Q[P(T)]$\n\n-\\> 마팅게일, 몬테카를로 시뮬레이션(Montecarlo simulation, MCS)을 주로 사용함\n\n## Volume과 적분\n\n$x\\sim uniform[0,1]\\;일 때,\\;\\;\\alpha=E[f(x)]=\\int_0^1f(x)dx$\n\n그러나, MCS를 이용하는 경우 임의변수 $x_1,x_2,...,x_n$을 샘플링하여 $\\hat{\\alpha}=\\frac{1}{N}\\sum_i^Nf(x_i)$로 산출함\n\n두 값이 정확히 일치하지는 않지만, 표본이 커질수록 그 오차는 0으로 수렴함($\\alpha\\approx\\hat{\\alpha}$)\n\n이는 **대수의 법칙과 중심극한정리**에 따라 수학적으로 정의할 수 있음\n\n::: {.callout-note title=\"중심극한정리\"}\n표본평균($\\hat{\\alpha}$)은 정규분포를 따르므로, $\\hat{\\alpha}-\\alpha\\sim N(0,\\frac{\\sigma^2}{N})$\n\n즉, 표본의 크기가 커질수록 두 차이는 0으로 수렴함(probibility convergence)\n\n오차의 표준편차는 $\\frac{\\sigma}{\\sqrt{N}}$이므로, 표본의 크기가 100배 증가하면 오차의 표준편차는 10배 감소함\n:::\n\n이외에도 간단힌 사다리꼴(trapezoidal) 방식을 이용해볼 수 있음.\n\n$3.\\;\\alpha\\approx \\frac{f(0)+f(1)}{2n}+\\frac{1}{n}\\sum_{i=1}^{n-1}f(\\frac{i}{n})\\;\\;(구분구적법의\\;앞뒤\\;평균치)$\n\n이는 매우 간단하고 효율적인 방법이지만, 변수가 늘어날 수록 효율이 급감함.\n\n## MCS 기초\n\n몬테카를로 시뮬레이션을 개념적으로 설명함\n\n예를 들어, 1X1 사각형에 내접한 원에 대하여, 사각형 안에 임의의 점을 찍을 때 원에 포함될 확률?\n\n직관적으로 면적을 통해 $\\Pi/4$임을 알 수 있음.\n\n이를 다변수, 다차원, 복잡한 함수꼴로 확장한다면 면적을 구하는 적분을 통해 구할 수 있음을 의미함.\n\n근데 그런 복잡한 계산 대신에 랜덤변수를 생성해서 시행횟수를 수없이 시행하고,\n\n원(면적) 안에 속할 확률을 구한다면? 이게 몬테카를로 시뮬레이션의 기초임.\n\n수없이 많은 $(x,y)$를 생성하고, 좌표평면의 1X1 사각형에 대해 원안에 속할 확률은 $x^2+y^2<1/4$임.\n\n이러한 확률을 구하는 것은 기대값으로 표현할 수 있게 되고, 결국 이 확률은 $\\Pi/4$로 수렴\n\n$$Pr(x\\in B)=E(\\int_A 1_B)=\\Pi/4$$\n\n### 확률기대값 및 원주율 계산 예시\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nn = 10000\nx = np.random.rand(n) # uniform random number in (0,1)\nx -= 0.5\ny = np.random.rand(n)\ny -= 0.5\n\nd = np.sqrt(x**2+y**2)\ni = d<0.5\nprob = i.sum() / n\npi = 4 * i.sum() / n\n\nprint(prob,pi,sep=\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.7841\n3.1364\n```\n:::\n:::\n\n\n### 표본표준편차 계산 : numpy는 n으로 나누고, pandas는 n-1로 나누는 것이 기본\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nnp_s = i.std()\npd_s = pd.Series(i).std()\nnp_s_df1 = i.std(ddof=1)\nprint(np_s, pd_s, np_s_df1, sep = \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.41144524544585515\n0.4114658192511757\n0.4114658192511757\n```\n:::\n:::\n\n\n### 표준오차 계산 및 95% 신뢰구간 계산\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nse = pd_s / np.sqrt(n)\nprob_95lb = prob - 2*se\nprob_95ub = prob + 2*se\npi_95lb = prob_95lb*4\npi_95ub = prob_95ub*4\nprint(se, pi_95lb, pi_95ub, sep=\"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.004114658192511757\n3.103482734459906\n3.1693172655400943\n```\n:::\n:::\n\n\n## 경로의존성 (Path-dependent)\n\n일반적인(Plain vanilla) 옵션은 pay-off가 기초자산의 만기시점의 가격 $S(T)$에 의해서만 결정되므로,\n\n그 사이의 기초자산의 가격을 생성할 필요는 없음(0~T)\n\n그러나, 아시안옵션 등은 $S(T)$ 뿐만 아니라 그 과정에 의해서 pay-off가 결정되므로 그 경로를 알아야 함.\n\n또한, 블랙숄즈의 가정이 성립하지 않는 경우 모델링을 하기 위해서도 그 경로를 알아야 할 필요가 있음.\n\n이를 **경로의존성**이라고 함.\n\n### 시뮬레이션 예시\n\n일반적인 주가에 대한 확률과정이 GBM을 따른다면,\n\n$dS(t)=rS(t)dt+\\sigma S(t)dW(t)$\n\n그러나, 변동성이 주가에 따라 변하면 주가의 흐름에 따라 변동성이 바뀌므로 경로의존성이 발생\n\n즉, $dS(t)=rS(t)dt+\\sigma (S(t)) S(t)dW(t)$를 따르게 되므로\n\n우리가 앞서 사용한 $S(T)=S(0)e^{(r-\\frac{1}{2}\\sigma^2)T+\\sigma\\sqrt{T}Z}$를 사용할 수 없음.\n\n따라서, Analytic solution이 없으므로 근사치를 구할 수 밖에 없으며 그 예시로 **이산오일러근사**가 있음\n\n(0~T) 구간을 m개로 나누고, 각 구간의 길이 $\\frac{T}{m}=\\Delta t$라고 하면 기초자산의 경로 $S(t)$는,\n\n$$S(t+\\Delta t)=S(t)+rS(t)\\Delta t+\\sigma (S(t)) S(t)\\sqrt{\\Delta t}Z$$\n\n다만, 이러한 경우에는 그 경로의 길이를 얼마나 짧게 구성하는지에 따라 시뮬레이션 정밀도에 영향을 미침.\n\n즉, 시뮬레이션 횟수 n과 경로의 길이 m이 모두 정확도를 결정하는 파라미터가 됨.\n\n## MCS 추정치 개선 방향\n\nMCS의 효율성은 아래 3개의 기준에 따라 평가할 수 있습니다.\n\n1. 계산시간 (Computing time)\n2. 편의 (Bias)\n3. 분산 (Variance)\n\n여기서, 시뮬레이션의 $Prediction\\;error\\;=\\;Variance\\;+\\;Bias^2$\n\n::: {.callout}\n$Var[\\epsilon]=E[\\epsilon^2]-(E[\\epsilon])^2$\n\n$MSE=E[\\epsilon^2)=Var[\\epsilon]+(E[\\epsilon])^2=Variance+Bias^2$\n:::\n\n### 분산감소와 계산시간\n\n시행횟수가 증가하면 분산은 감소함. ($n\\rightarrow\\infty,Var[\\epsilon]\\rightarrow 0$)\n\n한번의 시뮬레이션에 정확한방법을 사용할 수록 편의는 감소함($m\\rightarrow\\infty,Bias\\rightarrow 0$)\n\n(정확한방법을 사용할 수록 분산은 증가할 수 있음 (머신러닝 overfitting 같은 문제?))\n\n(정확한방법을 쓸수록 계산비용이 증가하여 시뮬레이션 횟수가 감소함, 분산이 그래서 증가함)\n\n#### 시뮬레이션의 횟수\n\n계산 예산에 $s$이고, 한번의 시뮬레이션의 계산량이 $\\tau$일 때, 가능한 시뮬레이션 횟수는 $s/\\tau$임\n\n이 때, 추정치의 분포 $\\sqrt{\\frac{s}{\\tau}}[\\hat{C}-C]\\rightarrow N(0,\\sigma_c^2)$\n\n$\\Rightarrow [\\hat{C}-C]\\rightarrow N(0,\\sigma_c^2(\\frac{\\tau}{c}))$ 이므로,\n\n계산오차는 분산이 $\\sigma_c^2(\\frac{tau}{c})$인 정규분포에 수렴함을 의미\n\n#### 편의\n\n경로의존성이 있는 시뮬레이션 중, 과거 연속적인 수치에 따라 pay-off가 정해진다면,\n\n이산오일러근사를 사용할 때 편의가 발생함.\n\n**e.g.** 룩백옵션의 경우 시뮬레이션이 항상 실제 pay-off를 과소평가 = (-) bias 존재\n\n이 때, 이산구간의 간격 m을 작게할 수록 편의는 감소함.\n\n또는, 기초자산이 비선형구조인 경우 등에도 편의가 발생할 수 있음.\n\n**e.g.** Compound 옵션의 경우 기초자산인 옵션 가격이 비선형이므로,\n\nCompound 옵션을 Analytic solution을 적용하여 푸는 경우 항상 실제 옵션보다 가격이 높음 = (+) bias 존재\n\n이 때, $T_1\\sim T_2$의 $n_2$개의 경로를 추가로 생성하여 경로를 이중으로 구성한다면 bias 제거가 가능함.\n\n## Asian Option 평가 해볼 것\n\n",
    "supporting": [
      "시뮬레이션1_files/figure-pdf"
    ],
    "filters": []
  }
}