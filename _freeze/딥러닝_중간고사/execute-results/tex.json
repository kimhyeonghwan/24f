{
  "hash": "93ae4d3a68bf96f5cfd3b383f05fe18c",
  "result": {
    "engine": "jupyter",
    "markdown": "# 전산금융 Mid-term {.unnumbered}\n\n20249132 김형환\n\n## Question 1\n\n![](image/deep_mid_1.png)\n\n### Answer\n\n먼저, 주어진 가중치 초기값 및 퍼셉트론 구조, 시그모이드 활성화 함수를 세팅하였습니다.\n\n문제의 훈련데이터를 이용하여 전향계산을 실시한 결과, 출력값은 약 [0.53, 0.63]이였으며,\n\n손실함수(오차제곱합/2)의 값은 0.31입니다.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\ndef sigmoid(x): return 1/(1+np.exp(-x))\ndef sigmoid_derivatives(x): return x * (1-x)\n\n# 뉴런a와 뉴런b 사이의 가중치는 wab. 뉴런a의 편향은 w0a\nw03, w04, w05, w06, w07 = 0.1, 0.2, 0.5, -0.1, 0.6\nw13, w14, w15 = 0.1, 0, 0.3\nw23, w24, w25 = -0.2, 0.2, -0.4\nw36, w46, w56 = -0.4, 0.1, 0.6\nw37, w47, w57 = 0.2,-0.1,-0.2\nx1, x2 = 0.6, 0.1\ny1, y2 = 1, 0\n\nx3 = sigmoid(w03 + w13*x1 + w23*x2)\nx4 = sigmoid(w04 + w14*x1 + w24*x2)\nx5 = sigmoid(w05 + w15*x1 + w25*x2)\n\nf1 = sigmoid(w06 + w36*x3 + w46*x4 + w56*x5)\nf2 = sigmoid(w07 + w37*x3 + w47*x4 + w57*x5)\n\nloss = 0.5 * (y1 - f1)**2 + 0.5 * (y2 - f2)**2\nprint(f1,f2,loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5335377723216794 0.6272786974969992 0.3055327870920766\n```\n:::\n:::\n\n\n이 값들을 이용해서 뉴런 2와 3의 가중치($w23$)의 업데이트를 하기 위해서는 오차역전파를 사용해야합니다.\n\n먼저 출력층과 은닉층 사이의 모든 계수를 먼저 업데이트하고, 이때 사용한 $\\delta$값들을 이용해서 은닉층과 입력층 사이의 계수를 업데이트하게 됩니다.\n\n각 $delta$는 출력층 및 은닉층의 활성화함수를 미분하여 계산할 수 있습니다.\n\n**이에 따라 업데이트한 $w23$의 값은 약 -0.2002입니다.**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndelta_f1 = (y1 - f1) * sigmoid_derivatives(f1)\ndelta_f2 = (y2 - f2) * sigmoid_derivatives(f2)\ndelta_x3 = (delta_f1 * w36 + delta_f2 * w37) * sigmoid_derivatives(x3)\n\nw23 += 0.1 * delta_x3 * x2\nw23\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n-0.20018849467653782\n```\n:::\n:::\n\n\n## Question 2\n\n![](image/deep_mid_2.png)\n\n### Answer\n\nReLu함수를 사용할 때, 그래디언트 소실 문제가 발생한 것이 원인입니다.\n\nReLu함수는 입력값이 음수일 때, 출력값이 모두 0이 되므로 음수인 구간에서 기울기가 0입니다.\n\n따라서 가중치가 제대로 업데이트되지 않으며, 이러한 경우를 \"죽은 ReLu문제\"라고 부릅니다.\n\n선택지 중 적절한 해법인 ***B. Leaky ReLu 함수를 사용한다.*** 입니다.\n\n이 활성화함수는 입력값이 음수인 경우에도 0.1의 기울기를 가지고 감소하므로, 그래디언트 소실 문제를 해결할 수 있습니다.\n\n## Question 3\n\n![](image/deep_mid_3.png)\n\n### Answer\n\n먼저, 배치 정규화의 목적은 내부 공변량 변화(Internal Covariate Shift)를 줄이는 것입니다.\n\n이를 위해 입력값을 표준화하는데, 모든 뉴런의 입력값의 분포를 평균=0, 표준편차=1로 고정시키게 됩니다.\n\n이 때, 입력값이 0 근방에 모여있는 경우 또다른 문제가 발생할 수 있습니다.\n\n활성화함수가 시그모이드 계열인 경우 0 부근에서 선형이고, ReLu 계열인 경우 음수인 경우 모두 0이기 때문입니다.\n\n따라서 활성화함수에 맞게 적절한 스케일링(γ)과 편향 이동(β)을 적용하여 학습이 잘 진행되도록 조정해야합니다.\n\n추가적으로, 스케일링과 편향 이동을 추가하면 모델이 표준화된 값들을 데이터의 형태에 따라 선형변환하여 학습하므로, 다양한 데이터 형태에 대응할 수 있는 장점도 있습니다.\n\n## Question 4\n\n![](image/deep_mid_4.png)\n\n### Answer\n\n가장 큰 문제는 가중치 초기화와 관련된 문제입니다.\n\n현재 가중치를 $N(0,100)$의 정규분포를 이용하여 초기화하고 있는데, 여기에서 추출된 난수는 표준편차가 10이므로 다소 큰 값의 가중치가 활성화 함수로 들어가게 됩니다.\n\n현재 활성화함수는 시그모이드 함수로, 가중치가 크거나 작으면 그 값이 1과 0으로 수렴하게 되는데, 이로 인해 일정 값 이상에서는 기울기가 거의 0으로 수렴하게 됩니다.\n\n이는 그래디언트 소실 문제로 이어지게 되며, 전향계산 후 역전파 과정에서 가중치 업데이트가 거의 이루어지지 않아 학습이 정상적으로 진행되지 않는 것 입니다.\n\n해결을 위해서는 가중치를 적절한 값으로 초기화해야하는데, 대표적인 예시로 Xavier초기화 방법을 시그모이드 계열에 적용할 수 있습니다.\n\n## Question 5\n\n![](image/deep_mid_5.png)\n\n### Answer\n\n데이터는 100개의 세트가 있고, 미니배치는 25개씩 구성됩니다. 한 에포크당 4번의 미니배치가 진행되고, 50 epoch 동안 반복하므로 총 200회 업데이트가 발생하게 됩니다. 따라서 가중치 $w_{31}^{(2)}$ 는 200회 업데이트됩니다.\n\n## Question 6\n\n(1) 아래 코드를 실행하여 fashion_MNIST 데이터에 대한 훈련, 평가 자료를 로딩하고, 전처리하여라.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import fashion_mnist\n\n(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\nx_train = x_train.reshape( 60000, 784 )\nx_test = x_test.reshape( 10000, 784 )\nx_train = x_train.astype( np.float32 )/255.0\nx_test = x_test.astype( np.float32 )/255.0\ny_train = tf.keras.utils.to_categorical( y_train, 10 )\ny_test = tf.keras.utils.to_categorical( y_test, 10 )\n```\n:::\n\n\n(2) 위에서 준비된 데이터에 적용할 모델은 4개의 은닉층을 가지는 다층퍼셉트론이며, 각 은닉층의 뉴런 수와 활성화 함수는 아래와 같이 정의되어야 한다.\n- 은닉층 1 : 1024개의 뉴런, ReLU 활성화 함수\n- 은닉층 2 : 512개의 뉴런, ReLU 활성화 함수\n- 은닉층 3 : 512개의 뉴런, ReLU 활성화 함수\n- 은닉층 4 : 512개의 뉴런, ReLU 활성화 함수\n또한 목표변수의 형태와 목적에 맞게 출력층과 손실함수를 설정하여라. 그 밖의 다른 훈련 기법들은 수업시간에 배운 방법들을 다양하게 시도해 보고 필요한 만큼 자유롭게 선택하여 적용하여라.\n\n**다항분류 문제이므로 출력층은 Softmax함수 및 크로스엔트로피 손실함수를 적용하였습니다.**\n\n**훈련 기법으로는, 활성화함수 전에 배치정규화, Drop-out(p=0.5), 학습률 스케줄링(모멘텀 optimizer)을 적용하였습니다.**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# 함수로 설정\n\ndef generate_model():\n    import tensorflow as tf\n    from tensorflow.keras.callbacks import EarlyStopping\n\n    model = tf.keras.Sequential()\n\n    # 은닉층 1\n    model.add(tf.keras.layers.Dense(units=1024,input_dim=x_train.shape[1],kernel_initializer='he_normal'))\n    model.add(tf.keras.layers.BatchNormalization())  # 배치 정규화 적용\n    model.add(tf.keras.layers.Activation('relu'))  # Relu 활성화 함수 적용\n    model.add(tf.keras.layers.Dropout(0.5))  # 0.5 drop-out 적용\n\n    # 은닉층 2\n    model.add(\n        tf.keras.layers.Dense(units=512,kernel_initializer='he_normal'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n\n    # 은닉층 3\n    model.add(tf.keras.layers.Dense(units=512,kernel_initializer='he_normal'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n\n    # 은닉층 4\n    model.add(tf.keras.layers.Dense(units=512,kernel_initializer='he_normal'))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Activation('relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n\n    # 출력층 : 9개의 범주를 가지는 다항범주형 변수이므로 softmax 회귀모형 적용\n    # 손실함수는 cross-entropy함수 적용 예정\n    model.add(\n        tf.keras.layers.Dense(\n            units=y_train.shape[1],\n            kernel_initializer='he_normal',\n            activation='softmax'))\n\n    return model\n```\n:::\n\n\n(3) 위 모델에 다양한 고속 옵티마이저를 각각 적용해 보아라. 훈련 자료 중 무작위로 선택된 20%의 검증자료를 이용하여 그 성능을 비교하여라. 이 때 성능 비교 결과는 학습곡선으로 정리할 것. 즉, 가로축은 에포크 횟수, 세로축은 검증 자료의 정확도(accuracy)가 되도록 하여 학습 곡선을 표현하되, 각 옵티마이저를 적용하였을 때의 결과를 서로 다른 색의 선그림으로 표현할것.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport time\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# 학습률 스케줄링 : 모멘텀 및 네스테로프 방식에 지수감쇄방식 적용\nlearning0 = 0.01\nlearning_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=learning0,\n    decay_steps=10000, decay_rate=0.5, staircase=True)\n\noptims = {\n    'Momentum': tf.keras.optimizers.SGD(momentum=0.9, learning_rate=learning_schedule),\n    'Nesterov': tf.keras.optimizers.SGD(momentum=0.9, learning_rate=learning_schedule, nesterov=True),\n    'AdaGrad': tf.keras.optimizers.Adagrad(),\n    'RMSProp': tf.keras.optimizers.RMSprop(),\n    'Adam': tf.keras.optimizers.Adam()\n}\n\nresult_optims = {}\nresult_time = {}\n\nfor name, optim in optims.items():\n    start = time.time()\n    model = generate_model()\n    model.compile(optimizer=optim, loss='categorical_crossentropy', metrics=['accuracy'])\n    result = model.fit(\n        x_train, y_train,\n        epochs=15, batch_size=64, verbose=0,\n        validation_split=0.2)\n    end = time.time()\n    result_optims[name] = result.history['val_accuracy']\n    result_time[name] = end - start\n\n# Plot accuracy comparison\nplt.figure(figsize=(12, 8))\nfor name, accuracy in result_optims.items():\n    plt.plot(range(1, len(accuracy)+1), accuracy, label=name)\n\nplt.title(\"Optimizers Performance\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Validation Accuracy\")\nplt.legend()\nplt.grid(True)\nplt.show()\nprint(pd.DataFrame(result_optims))\nprint(pd.DataFrame([result_time]))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/hwan/.pyenv/versions/hwan/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](딥러닝_중간고사_files/figure-pdf/cell-6-output-2.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n    Momentum  Nesterov   AdaGrad   RMSProp      Adam\n0   0.826250  0.831917  0.660333  0.850417  0.839417\n1   0.841417  0.847917  0.707167  0.854917  0.843000\n2   0.855000  0.852000  0.727500  0.865917  0.856000\n3   0.861333  0.866083  0.743083  0.869417  0.866917\n4   0.868333  0.867333  0.751833  0.856250  0.871667\n5   0.868500  0.870000  0.757500  0.876833  0.870750\n6   0.869917  0.871000  0.766583  0.869167  0.880500\n7   0.876250  0.875583  0.771833  0.878167  0.885500\n8   0.876417  0.876583  0.779167  0.874000  0.881083\n9   0.877250  0.875750  0.782833  0.884917  0.882500\n10  0.874667  0.877750  0.788167  0.884583  0.886083\n11  0.882750  0.884083  0.793083  0.880333  0.883667\n12  0.882000  0.882000  0.795000  0.881417  0.887250\n13  0.878917  0.884833  0.799083  0.880000  0.873750\n14  0.889333  0.888917  0.802000  0.871917  0.891417\n    Momentum   Nesterov    AdaGrad    RMSProp     Adam\n0  67.234614  83.133877  97.153556  94.216228  89.8862\n```\n:::\n:::\n\n\n(4) 위 결과를 이용하여 어느 옵티마이저가 가장 빠르고 정교하게 작동하는지에 관해 설명할 것.\n\n**AdaGrad 방식은 수렴속도가 느리고, 성능이 다른 옵티마이저에 비해 다소 떨어지는 것으로 나타났으며,**\n\n**그래프 및 표를 보았을 때 Adam 방식의 옵티마이저가 가장 빠르고 정교하게 작동하는 것으로 보입니다.**\n\n**다만, 학습 시간에 있어서 지수 감쇄 학습률을 적용한 모멘텀 방식이 약 20~30% 빨리 학습되므로,**\n\n**상황에 따라 적절한 옵티마이저를 선택하는 것이 필요합니다.**\n\n(5) 가장 성능이 좋은 옵티마이저를 적용한 모델을 최종적으로 학습한 뒤 (1)에서 준비해둔 평가 자료에 적용한 뒤 평가 정확도(accuracy)를 구하여라.\n\n**저는 지수 감쇄 학습률을 적용한 모멘텀 최적화가 수렴 속도 및 안정성, 계산예산을 종합적으로 고려할 때**\n\n**가장 성능이 우수하다고 생각하였습니다. 이를 이용한 평가 정확도는 아래와 같습니다.**\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nlast_model = generate_model()\nlast_model.compile(\n    optimizer=tf.keras.optimizers.SGD(momentum=0.9, learning_rate=learning_schedule),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'])\n\nlast_model.fit(x_train, y_train, epochs=30, batch_size=64, verbose=0)\nlast_result = last_model.evaluate(x_test, y_test, verbose=0)\nprint('Model Accuracy is :', last_result[1])\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/hwan/.pyenv/versions/hwan/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning:\n\nDo not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nModel Accuracy is : 0.891700029373169\n```\n:::\n:::\n\n\n",
    "supporting": [
      "딥러닝_중간고사_files/figure-pdf"
    ],
    "filters": []
  }
}